# Web Voice Mode ‚Äî Conversation Mode with AI Agent Icon

**Phase**: Feature Addition  
**Status**: Implemented  
**Flag**: `FEATURE_VOICE_WEB` (default: `false` ‚Äî opt-in)

---

## Overview

Web Voice Mode adds a **single AI agent icon** that activates a hands-free conversation loop combining **speech-to-text (STT)** and **neural text-to-speech (TTS)**. One tap starts a natural back-and-forth conversation with the AI receptionist.

- **STT**: MediaRecorder ‚Üí backend `/api/stt` ‚Üí OpenAI Whisper ‚Üí transcript auto-sent
- **TTS**: Frontend fetches `/api/tts` ‚Üí backend OpenAI TTS (tts-1) ‚Üí audio streamed to browser `Audio` element
- **Silence Detection**: Web Audio API AnalyserNode auto-stops recording after **3s** of silence
- **Conversation Mode**: Tap agent icon ‚Üí speak ‚Üí 3s pause ‚Üí transcribe ‚Üí send ‚Üí AI speaks reply ‚Üí auto-restart recording ‚Üí loop
- **Exit**: Tap agent icon again, or 3s silence with no speech exits conversation mode
- **No audio stored on disk** ‚Äî streamed directly, discarded after use

---

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Widget (Vite ‚Äî port 5173)                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ ChatWidget   ‚îÇ  ‚îÇ useVoice hook        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ ü§ñ agent   ‚îÇ‚îÄ‚îÄ‚îÇ ‚Ä¢ conversationMode   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   icon btn   ‚îÇ  ‚îÇ ‚Ä¢ MediaRecorder      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ status bar ‚îÇ  ‚îÇ ‚Ä¢ fetch /api/stt     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ ‚Ä¢ fetch /api/tts     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ ‚Ä¢ Audio element      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ ‚Ä¢ Silence detection   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ ‚Ä¢ 3s silence timeout  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ POST /api/stt           ‚îÇ POST /api/tts
      ‚îÇ (multipart audio)       ‚îÇ (JSON {text})
      ‚ñº                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Backend (Fastify ‚Äî port 3000)              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ stt.routes.ts    ‚îÇ  ‚îÇ tts.routes.ts  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ multipart      ‚îÇ  ‚îÇ ‚Ä¢ JSON body    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Whisper API    ‚îÇ  ‚îÇ ‚Ä¢ ttsProvider  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ ttsProvider.ts                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ OpenAI TTS (tts-1 / tts-1-hd)     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Text preprocessing                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Sentence chunking                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Voice selection                    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  Feature-gated: FEATURE_VOICE_WEB           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Environment Variables

| Variable | Default | Description |
|---|---|---|
| `FEATURE_VOICE_WEB` | `false` | Master kill switch. When `true`, `/api/stt` and `/api/tts` are registered and the mic button appears. |
| `TTS_VOICE` | `nova` | Neural TTS voice. Options: `alloy`, `ash`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, `shimmer` |
| `TTS_MODEL` | `tts-1` | TTS model. `tts-1` (fast, lower latency) or `tts-1-hd` (higher quality) |

> **Note**: Requires `OPENAI_API_KEY` to be set (same key used by chat). No additional API keys needed.

---

## Files Changed / Created

### Backend
| File | Change |
|---|---|
| `src/backend/src/config/env.ts` | Added `FEATURE_VOICE_WEB`, `TTS_VOICE`, `TTS_MODEL` to Zod schema |
| `src/backend/src/config/capabilities.ts` | Added `voiceWeb` to `AppCapabilities` interface + `deriveCapabilities()` |
| `src/backend/src/routes/stt.routes.ts` | **NEW** ‚Äî POST `/api/stt` route (multipart audio ‚Üí Whisper ‚Üí transcript) |
| `src/backend/src/routes/tts.routes.ts` | **NEW** ‚Äî POST `/api/tts` route (JSON text ‚Üí OpenAI TTS ‚Üí audio buffer) |
| `src/backend/src/voice/ttsProvider.ts` | **NEW** ‚Äî TTS provider abstraction (OpenAI TTS, text preprocessing, chunking) |
| `src/backend/src/index.ts` | Import + register `sttRoutes` and `ttsRoutes` (gated by `FEATURE_VOICE_WEB`) |
| `src/backend/.env` | Added `FEATURE_VOICE_WEB=true`, `TTS_VOICE=nova`, `TTS_MODEL=tts-1` |
| `src/backend/.env.example` | Added documentation for voice config variables |
| `src/backend/package.json` | Added `@fastify/multipart` dependency |
| `src/backend/tests/capabilities.test.ts` | Added `voiceWeb` to all expected shapes + 3 new tests |
| `src/backend/tests/feature-flags.test.ts` | Added `FEATURE_VOICE_WEB` schema validation test |
| `src/backend/tests/tts-route.test.ts` | **NEW** ‚Äî TTS route tests (feature gate, validation, synthesis, preprocessing) |

### Frontend (Widget)
| File | Change |
|---|---|
| `src/frontend/src/hooks/useVoice.ts` | **NEW** ‚Äî React hook: state machine, MediaRecorder, silence detection, neural TTS via `/api/tts` + Audio element |
| `src/frontend/src/hooks/useCapabilities.ts` | Added `voiceWeb` to `AppCapabilities` interface |
| `src/frontend/src/components/ChatWidget.tsx` | Added mic button, auto-speak toggle, voice status bar, auto-speak on new assistant message |

---

## How It Works

### Conversation Mode Flow
1. User taps the **AI agent icon** (human silhouette SVG) in the input row
2. `conversationMode` activates ‚Üí `autoSpeak` turns ON ‚Üí `startRecording()` begins
3. `MediaRecorder` captures audio chunks (WebM/Opus preferred)
4. Web Audio API `AnalyserNode` monitors RMS volume level
5. When silence detected for **3s** (after speech) ‚Üí auto-stop recording
6. Blob sent to `POST /api/stt` as `multipart/form-data`
7. Backend streams to OpenAI Whisper (`whisper-1`), returns `{ transcript }`
8. Transcript auto-sent via `sendMessageRef.current(text)` ‚Äî hands-free flow
9. Assistant response arrives ‚Üí TTS auto-speaks the reply (neural voice)
10. TTS finishes (`audio.onended`) ‚Üí auto-restart recording ‚Üí **loop back to step 3**
11. **Exit**: User taps agent icon again, OR 3s silence without any speech ‚Üí conversation mode exits

### Neural TTS (Auto-Speak)
1. Auto-speak is automatically enabled when entering conversation mode
2. When new assistant message arrives:
   a. `voice.speak(text)` ‚Üí `preprocessForSpeech()` strips code blocks, markdown
   b. `POST /api/tts` with `{ text }` ‚Üí backend calls OpenAI TTS API
   c. Response: raw audio bytes (`audio/mpeg`) ‚Üí blob ‚Üí `URL.createObjectURL()`
   d. `new Audio(blobUrl).play()` ‚Üí natural-sounding neural speech
3. **Barge-in**: starting a new recording stops audio playback immediately
4. After TTS finishes, recording auto-restarts (in conversation mode)

### State Machine
```
idle ‚Üí recording ‚Üí transcribing ‚Üí idle
                ‚Üò error ‚Üí idle (auto-recover 3s)

idle ‚Üí speaking ‚Üí idle
     (barge-in cancels)
```

---

## API Reference

### POST /api/stt

**Content-Type**: `multipart/form-data`

| Field | Type | Required | Description |
|---|---|---|---|
| `audio` | File | ‚úÖ | Audio recording (max 25 MB) |

**Accepted MIME types**: `audio/webm`, `audio/wav`, `audio/mp4`, `audio/mpeg`, `audio/ogg`, `audio/flac`

**Success Response** (200):
```json
{ "transcript": "I'd like to book an appointment for Tuesday" }
```

**Error Responses**:
- `400` ‚Äî No file, empty file, or unsupported format
- `404` ‚Äî `FEATURE_VOICE_WEB` not enabled
- `413` ‚Äî File exceeds 25 MB
- `502` ‚Äî Whisper transcription failed

### POST /api/tts

**Content-Type**: `application/json`

| Field | Type | Required | Description |
|---|---|---|---|
| `text` | string | ‚úÖ | Text to synthesize (1‚Äì4096 chars after preprocessing) |
| `voice` | string | ‚ùå | Voice override: `alloy`, `ash`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, `shimmer` |
| `format` | string | ‚ùå | Output format: `mp3` (default) or `wav` |

**Text Preprocessing** (server-side):
- Fenced code blocks (` ```...``` `) are stripped
- Inline code backticks are removed
- Excess whitespace is collapsed

**Success Response** (200):
- `Content-Type`: `audio/mpeg` or `audio/wav`
- `Cache-Control`: `no-store`
- Body: raw audio bytes

**Error Responses**:
- `400` ‚Äî Empty text, text-only code blocks, or text exceeds 4096 chars
- `404` ‚Äî `FEATURE_VOICE_WEB` not enabled
- `502` ‚Äî OpenAI TTS API error

### GET /api/tts/voices

Returns available TTS voices and current defaults.

**Success Response** (200):
```json
{
  "voices": ["alloy", "ash", "coral", "echo", "fable", "nova", "onyx", "sage", "shimmer"],
  "default": "nova",
  "model": "tts-1"
}
```

**Error Responses**:
- `404` ‚Äî `FEATURE_VOICE_WEB` not enabled

---

## Manual Test Checklist

### Prerequisites
- [ ] Backend running (`npm run dev` in `src/backend/`)
- [ ] Widget running (`npm run dev` in `src/frontend/`)
- [ ] `FEATURE_VOICE_WEB=true` in `src/backend/.env`
- [ ] `OPENAI_API_KEY` set with valid key
- [ ] HTTPS or localhost (mic requires secure context)

### Feature Flag Gating
- [ ] With `FEATURE_VOICE_WEB=false`: no agent icon visible, POST `/api/stt` and `/api/tts` return 404
- [ ] With `FEATURE_VOICE_WEB=true`: agent icon visible, routes respond

### Conversation Mode
- [ ] Click the AI agent icon (human silhouette) ‚Üí browser asks for mic permission
- [ ] Grant permission ‚Üí icon glows green, status bar shows "üéôÔ∏è Listening‚Ä¶"
- [ ] Speak a message ‚Üí pause for 3 seconds
- [ ] Auto-stops recording, status bar shows "‚è≥ Processing‚Ä¶"
- [ ] Transcript auto-sent into chat ‚Äî message appears immediately
- [ ] Assistant reply is auto-spoken with neural TTS
- [ ] After TTS finishes, recording auto-restarts (conversation loop)
- [ ] Click agent icon again ‚Üí conversation mode exits, everything stops
- [ ] 3s silence without speaking ‚Üí exits conversation mode automatically
- [ ] Click ‚úï in status bar ‚Üí ends conversation mode

### Neural TTS
- [ ] In conversation mode, assistant replies are spoken automatically
- [ ] Sound quality is natural-sounding neural voice
- [ ] Start speaking while AI is talking ‚Üí speech stops immediately (barge-in)
- [ ] Code blocks in assistant replies are not spoken (stripped)

### Error Handling
- [ ] Deny mic permission ‚Üí error shown, auto-recovers in 3s
- [ ] Send empty recording ‚Üí no transcript injected
- [ ] Backend down ‚Üí "Transcription failed" error, auto-recovers

### Capabilities Endpoint
- [ ] `GET /api/capabilities` returns `"voiceWeb": true` when enabled
- [ ] `GET /api/capabilities` returns `"voiceWeb": false` when disabled

---

## Reversal

To disable Web Voice Mode completely:

1. Set `FEATURE_VOICE_WEB=false` in `src/backend/.env`
2. Restart the backend

The mic button will disappear, the `/api/stt` and `/api/tts` routes won't be registered, and the `voiceWeb` capability will report `false`. No code removal needed.

To remove the code entirely, delete:
- `src/backend/src/routes/stt.routes.ts`
- `src/backend/src/routes/tts.routes.ts`
- `src/backend/src/voice/ttsProvider.ts`
- `src/frontend/src/hooks/useVoice.ts`
- Voice-related sections in `ChatWidget.tsx` (search for `voiceEnabled` / `voiceStyles`)
- `FEATURE_VOICE_WEB`, `TTS_VOICE`, `TTS_MODEL` from `env.ts`, `capabilities.ts`, `.env`, `.env.example`
- `voiceWeb` from `useCapabilities.ts` interface
- `@fastify/multipart` from `package.json`
- `src/backend/tests/tts-route.test.ts`
